{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "\n",
    "with open('text.txt','r',encoding='utf-8') as f:\n",
    "  text = f.read()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# 输入的文本\n",
    "text = \"\"\"People who truly loved once are far more likely to love again.\n",
    "Difficult circumstances serve as a textbook of life for people.\n",
    "The best preparation for tomorrow is doing your best today.\n",
    "The reason why a great man is great is that he resolves to be a great man.\n",
    "The shortest way to do many things is to only one thing at a time.\n",
    "Only they who fulfill their duties in everyday matters will fulfill them on great occasions.\n",
    "I go all out to deal with the ordinary life.\n",
    "I can stand up once again on my own.\n",
    "Never underestimate your power to change yourself.\"\"\"\n",
    "\n",
    "def processText(text):\n",
    "    text = text.lower()\n",
    "    remove_chars = '[·’!\"\\#$%&\\'()＃！（）*+,-./:;<=>?\\@，：?￥★、…．＞【】［］《》？“”‘’\\[\\\\]^_`{|}~]+' # 去除标点符号\n",
    "    text= re.sub(remove_chars, \"\", text)\n",
    "    text = text.split()\n",
    "    return text\n",
    "\n",
    "\n",
    "text = processText(text)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "\n",
    "# 输入是1*V的矩阵，V是词汇表的大小\n",
    "EMDEDDING_DIM = 100  # 词向量维度, 词向量的维度一般是50, 100, 200, 300维, W1矩阵的列数\n",
    "\n",
    "word = set(text)\n",
    "word_size = len(word) # 词汇表的大小\n",
    "\n",
    "# 有了这两个映射，我们才能通过输出层预测结果\n",
    "word_to_ix = {word: ix for ix, word in enumerate(word)}\n",
    "ix_to_word = {ix: word for ix, word in enumerate(word)}\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![img](https://img-blog.csdnimg.cn/img_convert/466542a287c87cb2813cdad5cfc46fc9.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# 定义一个函数，把文本转换成索引的形式\n",
    "def make_context_vector(context, word_to_ix): # context是上下文单词列表，word_to_ix是单词到索引的映射\n",
    "    idxs = [word_to_ix[w] for w in context] # 将上下文单词转换成索引的形式\n",
    "    res = torch.tensor(idxs, dtype=torch.long)\n",
    "    return  res # 返回的是tensor\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# 定义训练数据\n",
    "data = [] # 定义一个列表，用来存储训练数据\n",
    "for i in range(2, len(text) - 2):\n",
    "    context = [text[i - 2], text[i - 1], # 根据上下文预测目标词汇，窗口大小为4\n",
    "               text[i + 1], text[i + 2]]\n",
    "    target = text[i]\n",
    "    data.append((context, target))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# 定义模型\n",
    "class CBOW(torch.nn.Module):\n",
    "    def __init__(self, word_size, embedding_dim):         # wordsize是词汇表的大小，embedding_dim是词向量的维度\n",
    "        super(CBOW, self).__init__()\n",
    "\n",
    "        self.embeddings = nn.Embedding(word_size, embedding_dim) # 定义词向量矩阵\n",
    "        self.linear1 = nn.Linear(embedding_dim, 128) # 定义第一个线性层\n",
    "        self.activation_function1 = nn.ReLU() # 定义激活函数，这里用的是ReLU\n",
    "        self.linear2 = nn.Linear(128, word_size) #  定义第二个线性层\n",
    "        self.activation_function2 = nn.LogSoftmax(dim=-1) # 定义激活函数，这里用的是LogSoftmax，LogSoftmax可以理解为归一化的概率, dim=-1表示最后一维, 这里是列\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = sum(self.embeddings(inputs)).view(1, -1) # 将上下文词向量求和，然后转换成1*V的形式\n",
    "        out = self.linear1(embeds) # 1*V 乘以 V*128 得到 1*128\n",
    "        out = self.activation_function1(out) # 1*128 经过激活函数,\n",
    "        out = self.linear2(out) # 1*128 乘以 128*V 得到 1*V\n",
    "        out = self.activation_function2(out) # 1*V 经过激活函数, 得到归一化的概率\n",
    "        return out\n",
    "\n",
    "    def get_word_emdedding(self, word):\n",
    "        word = torch.tensor([word_to_ix[word]]) # 将单词转换成索引的形式, 例如\"people\"转换成tensor([0])\n",
    "        return self.embeddings(word).view(1, -1) # 返回词向量\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# 初始化模型\n",
    "model = CBOW(word_size, EMDEDDING_DIM) # 词汇表的大小是word_size, 词向量的维度是EMDEDDING_DIM\n",
    "\n",
    "loss_function = nn.NLLLoss() # 定义损失函数，这里用的是NLLLoss，负对数似然损失函数,NLLLoss的输入是一个对数概率向量和一个目标标签, 它不会为我们计算对数概率, 需要我们自己计算好对数概率\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 35.73it/s, epoch=99, total_loss=9.61]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 开始训练\n",
    "epochs = tqdm(range(100))\n",
    "for epoch in epochs:\n",
    "    total_loss = 0\n",
    "\n",
    "    for context, target in data:\n",
    "        context_vector = make_context_vector(context, word_to_ix)\n",
    "\n",
    "        log_probs = model(context_vector)\n",
    "\n",
    "        total_loss += loss_function(log_probs, torch.tensor([word_to_ix[target]]))\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    epochs.set_postfix(epoch=epoch,total_loss=total_loss.item())\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "文本数据: people who truly loved once are far more likely to love again difficult circumstances serve as a textbook of life for people the best preparation for tomorrow is doing your best today the reason why a great man is great is that he resolves to be a great man the shortest way to do many things is to only one thing at a time only they who fulfill their duties in everyday matters will fulfill them on great occasions i go all out to deal with the ordinary life i can stand up once again on my own never underestimate your power to change yourself\n",
      "\n",
      "预测1: ['preparation', 'for', 'is', 'doing']\n",
      "\n",
      "预测结果: tomorrow\n",
      "\n",
      "\n",
      "预测2: ['the', 'reason', 'a', 'great']\n",
      "\n",
      "预测结果: why\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 预测\n",
    "context1 = ['preparation', 'for', 'is', 'doing']\n",
    "context_vector1 = make_context_vector(context1, word_to_ix)\n",
    "a = model(context_vector1)\n",
    "print(word_to_ix['reason'])\n",
    "context2 = ['the', 'reason', 'a', 'great']\n",
    "context_vector2 = make_context_vector(context2, word_to_ix)\n",
    "b = model(context_vector2)\n",
    "\n",
    "print(f'文本数据: {\" \".join(text)}\\n')\n",
    "print(f'预测1: {context1}\\n')\n",
    "print(f'预测结果: {ix_to_word[torch.argmax(a[0]).item()]}') # torch.argmax(a[0])返回的是a[0]中最大值的索引, torch.argmax(a[0]).item()返回的是a[0]中最大值的索引的值\n",
    "print('\\n')\n",
    "print(f'预测2: {context2}\\n')\n",
    "print(f'预测结果: {ix_to_word[torch.argmax(b[0]).item()]}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
